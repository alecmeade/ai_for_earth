{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "# To view tensorboard metrics\n",
    "# tensorboard --logdir=logs --port=6006 --bind_all\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "from evolver import CrossoverType, MutationType, MatrixEvolver\n",
    "from unet import UNet\n",
    "from dataset_utils import PartitionType\n",
    "from cuda_utils import maybe_get_cuda_device, clear_cuda\n",
    "from landcover_dataloader import get_landcover_dataloaders\n",
    "\n",
    "from ignite.contrib.handlers.tensorboard_logger import *\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, ConfusionMatrix\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.utils import setup_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories for data, logging and model saving.\n",
    "base_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(base_dir, \"data/landcover_small\")\n",
    "log_dir = os.path.join(base_dir, \"logs\")\n",
    "model_dir = os.path.join(base_dir, \"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for each partition of Landcover data.\n",
    "dataloader_params = {\n",
    "    'batch_size': 16,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True}\n",
    "\n",
    "partition_types = [PartitionType.TRAIN, PartitionType.VALIDATION, \n",
    "                   PartitionType.FINETUNING, PartitionType.TEST]\n",
    "data_loaders = get_landcover_dataloaders(dataset_dir, \n",
    "                                         partition_types,\n",
    "                                         dataloader_params)\n",
    "train_loader = data_loaders[0]\n",
    "validation_loader = data_loaders[1]\n",
    "finetuning_loader = data_loaders[2]\n",
    "test_loader = data_loaders[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU device if available.\n",
    "device = maybe_get_cuda_device()\n",
    "\n",
    "params = {\n",
    "    'max_epochs': 1,\n",
    "    'n_classes': 4,\n",
    "    'in_channels': 4,\n",
    "    'depth': 4,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.8,\n",
    "    'log_steps': 1,\n",
    "    'save_top_n_models': 3\n",
    "}\n",
    "\n",
    "clear_cuda()    \n",
    "model = UNet(in_channels = params['in_channels'],\n",
    "             n_classes = params['n_classes'],\n",
    "             depth = params['depth'])\n",
    "\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=params['learning_rate'],\n",
    "                            momentum=params['momentum'])\n",
    "\n",
    "\n",
    "metrics = {\"accuracy\": Accuracy(), \n",
    "           \"loss\": Loss(criterion)\n",
    "           \"confusion_matrix\": ConfusionMatrix(num_classes = params['n_classes']),\n",
    "          }\n",
    "metrics['mean_iou'] = mIoU(metrics['confusion_matrix')\n",
    "\n",
    "trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\n",
    "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "\n",
    "trainer.logger = setup_logger(\"Trainer\")\n",
    "train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "validation_evaluator.logger = setup_logger(\"Validation Evaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 01:46:48,982 Trainer INFO: Engine run starting with max_epochs=1.\n",
      "2020-07-28 01:54:49,675 Train Evaluator INFO: Engine run starting with max_epochs=1.\n"
     ]
    }
   ],
   "source": [
    "@trainer.on(Events.EPOCH_COMPLETED)\n",
    "def compute_metrics(engine):\n",
    "    \"\"\"Callback to compute metrics on the train and validation data.\"\"\"\n",
    "    train_evaluator.run(train_loader)\n",
    "    validation_evaluator.run(validation_loader)\n",
    "\n",
    "def score_function(engine):\n",
    "    \"\"\"Function to determine the metric upon which to compare model.\"\"\"\n",
    "    return engine.state.metrics[\"accuracy\"]\n",
    "    \n",
    "tb_logger = TensorboardLogger(log_dir=log_dir)\n",
    "\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED(every=params['log_steps']),\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batchloss\": loss},\n",
    "    metric_names=\"all\",\n",
    ")\n",
    "\n",
    "for tag, evaluator in [(\"training\", train_evaluator), (\"validation\", validation_evaluator)]:\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=tag,\n",
    "        metric_names=[\"loss\", \"accuracy\"],\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    log_dir,\n",
    "    n_saved=params['save_top_n_models'],\n",
    "    filename_prefix=\"best\",\n",
    "    score_function=score_function,\n",
    "    score_name=\"validation_accuracy\",\n",
    "    global_step_transform=global_step_from_engine(trainer),\n",
    ")\n",
    "validation_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})\n",
    "\n",
    "# kick everything off\n",
    "trainer.run(train_loader, max_epochs=params['max_epochs'])\n",
    "\n",
    "tb_logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
