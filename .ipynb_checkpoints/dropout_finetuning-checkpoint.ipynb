{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from evolver import CrossoverType, MutationType, MatrixEvolver\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tif_to_np(tif_path):\n",
    "    \"\"\"Reads a tif file and converts it into a numpy.ndarray.\n",
    "    \n",
    "    Arg:\n",
    "        tif_path: The full path to the tif file to read.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy.ndarray containing the tif file data. The returned tif has a rolled\n",
    "        dimension and so the input is in the shape (channels, height width).\n",
    "    \n",
    "    \"\"\"\n",
    "    with rasterio.open(tif_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "def apply_remap_values(labels, label_map):\n",
    "    \"\"\"Reassigns values inplace in an numpy array given a provided mapping.\n",
    "    \n",
    "    Args:\n",
    "        labels: An ndarray of labels.\n",
    "        label_map: A dict[int, int] mapping label classes [original, new].\n",
    "        \n",
    "    \"\"\"\n",
    "    for l1, l2 in label_map.items():\n",
    "        labels[labels == l1] = l2\n",
    "\n",
    "def sample_patch_coordinates(data, labels, patch_size, n_samples):\n",
    "    \"\"\"Generates image patches from a tile containing features and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        data: The x features of the image.\n",
    "        labels: The y labels of the image.\n",
    "        patch_size: An Iterable[int, int] size of the image patch to be extracted.\n",
    "        n_samples: The number of samples to extract per tile.\n",
    "\n",
    "    Returns:\n",
    "        A list of x_patches and y_patches containg features and labels respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    height, width = patch_size\n",
    "    channels = data.shape[0]\n",
    "    xs = np.random.randint(0, data.shape[2] - width, n_samples)\n",
    "    ys = np.random.randint(0, data.shape[1] - height, n_samples)\n",
    "    return np.dstack((xs, ys)).reshape((n_samples, 2))\n",
    "    \n",
    "class LandCoverDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Land Cover Dataset Containing patches. Loads a given tile into memory and slices it upon request.\"\"\"\n",
    "\n",
    "    def __init__(self, features_path, labels_path, patch_size, n_samples, patch_coordinates=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features_path: Path to the features of a tile.\n",
    "            labels_path: Path to the labels of a tile.\n",
    "            patch_size: An Iterable[int, int] size of the image patch to be extracted.\n",
    "            n_samples: The number of samples to extract per tile.\n",
    "            patch_coordinates: A list of coordinates used to identify the top left hand corners of\n",
    "                the patches to extract from the tile. If None they are randomly generated.\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = read_tif_to_np(features_path)\n",
    "        self.labels = read_tif_to_np(labels_path)\n",
    "        self.labels = self.labels - 1\n",
    "        \n",
    "        # Coalesces labels into 4 groups instead of 6.\n",
    "        # TODO(ameade): Consider allowing for transformation function arguments to modify data upon\n",
    "        # reading it in.\n",
    "        water_forest_land_impervious_remap = {1: 0, 2: 1, 3: 2, 4: 3, 5: 3, 6: 3}\n",
    "        apply_remap_values(self.labels, water_forest_land_impervious_remap)\n",
    "\n",
    "        self.n_classes = len(np.unique(self.labels))\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.n_samples = n_samples    \n",
    "        self.patch_coordinates = patch_coordinates\n",
    "        \n",
    "        if self.patch_coordinates is None:\n",
    "            self.patch_coordinates = sample_patch_coordinates(self.data, \n",
    "                                                              self.labels,\n",
    "                                                              self.patch_size,\n",
    "                                                              self.n_samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        height, width = self.patch_size\n",
    "        x, y = self.patch_coordinates[idx]\n",
    "        img = torch.from_numpy(self.data[:, y : y + height, x : x + width].astype(np.float32))\n",
    "        # Use LongTesnor cast for categorical.\n",
    "        label = torch.from_numpy(self.labels[0, y : y + height, x : x + width]).type(torch.LongTensor)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 10\n",
    "patch_size = (256, 256)\n",
    "samples_per_tile = 5000\n",
    "\n",
    "# Data Generators\n",
    "train_x_path = \"/mnt/blobfuse/esri-naip/v002/md/2015/md_100cm_2015/39076/m_3907639_sw_18_1_20150815.tif\"\n",
    "train_y_path = \"/mnt/blobfuse/resampled-lc/data/v1/2015/states/md/md_1m_2015/39076/m_3907639_sw_18_1_20150815_lc.tif\"\n",
    "\n",
    "test_x_path = \"/mnt/blobfuse/esri-naip/v002/md/2015/md_100cm_2015/39076/m_3907639_ne_18_1_20150815.tif\"\n",
    "test_y_path = \"/mnt/blobfuse/resampled-lc/data/v1/2015/states/md/md_1m_2015/39076/m_3907639_ne_18_1_20150815_lc.tif\"\n",
    "\n",
    "train_set = LandCoverDataset(train_x_path, train_y_path, patch_size, samples_per_tile)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "\n",
    "test_set = LandCoverDataset(test_x_path, test_y_path, patch_size, samples_per_tile)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model Loss and Optimizers\n",
    "net = UNet(in_channels = 4, n_classes = train_set.n_classes, depth = 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_x, batch_y = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_mask_evolver = MatrixEvolver([[3, 3]], CrossoverType.UNIFORM, MutationType.FLIP_BIT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
