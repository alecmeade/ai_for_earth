{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from evolver import CrossoverType, MutationType, MatrixEvolver\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tif_to_np(tif_path):\n",
    "    \"\"\"Reads a tif file and converts it into a numpy.ndarray.\n",
    "    \n",
    "    Arg:\n",
    "        tif_path: The full path to the tif file to read.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy.ndarray containing the tif file data. The returned tif has a rolled\n",
    "        dimension and so the input is in the shape (channels, height width).\n",
    "    \n",
    "    \"\"\"\n",
    "    with rasterio.open(tif_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "def apply_remap_values(labels, label_map):\n",
    "    \"\"\"Reassigns values inplace in an numpy array given a provided mapping.\n",
    "    \n",
    "    Args:\n",
    "        labels: An ndarray of labels.\n",
    "        label_map: A dict[int, int] mapping label classes [original, new].\n",
    "        \n",
    "    \"\"\"\n",
    "    for l1, l2 in label_map.items():\n",
    "        labels[labels == l1] = l2\n",
    "\n",
    "def sample_patch_coordinates(data, labels, patch_size, n_samples):\n",
    "    \"\"\"Generates image patches from a tile containing features and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        data: The x features of the image.\n",
    "        labels: The y labels of the image.\n",
    "        patch_size: An Iterable[int, int] size of the image patch to be extracted.\n",
    "        n_samples: The number of samples to extract per tile.\n",
    "\n",
    "    Returns:\n",
    "        A list of x_patches and y_patches containg features and labels respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    height, width = patch_size\n",
    "    channels = data.shape[0]\n",
    "    xs = np.random.randint(0, data.shape[2] - width, n_samples)\n",
    "    ys = np.random.randint(0, data.shape[1] - height, n_samples)\n",
    "    return np.dstack((xs, ys)).reshape((n_samples, 2))\n",
    "    \n",
    "class LandCoverDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Land Cover Dataset Containing patches. Loads a given tile into memory and slices it upon request.\"\"\"\n",
    "\n",
    "    def __init__(self, features_path, labels_path, patch_size, n_samples, patch_coordinates=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features_path: Path to the features of a tile.\n",
    "            labels_path: Path to the labels of a tile.\n",
    "            patch_size: An Iterable[int, int] size of the image patch to be extracted.\n",
    "            n_samples: The number of samples to extract per tile.\n",
    "            patch_coordinates: A list of coordinates used to identify the top left hand corners of\n",
    "                the patches to extract from the tile. If None they are randomly generated.\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = read_tif_to_np(features_path)\n",
    "        self.labels = read_tif_to_np(labels_path)\n",
    "        self.labels = self.labels - 1\n",
    "        \n",
    "        # Coalesces labels into 4 groups instead of 6.\n",
    "        # TODO(ameade): Consider allowing for transformation function arguments to modify data upon\n",
    "        # reading it in.\n",
    "        water_forest_land_impervious_remap = {1: 0, 2: 1, 3: 2, 4: 3, 5: 3, 6: 3}\n",
    "        apply_remap_values(self.labels, water_forest_land_impervious_remap)\n",
    "\n",
    "        self.n_classes = len(np.unique(self.labels))\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.n_samples = n_samples    \n",
    "        self.patch_coordinates = patch_coordinates\n",
    "        \n",
    "        if self.patch_coordinates is None:\n",
    "            self.patch_coordinates = sample_patch_coordinates(self.data, \n",
    "                                                              self.labels,\n",
    "                                                              self.patch_size,\n",
    "                                                              self.n_samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        height, width = self.patch_size\n",
    "        x, y = self.patch_coordinates[idx]\n",
    "        img = torch.from_numpy(self.data[:, y : y + height, x : x + width].astype(np.float32))\n",
    "        # Use LongTesnor cast for categorical.\n",
    "        label = torch.from_numpy(self.labels[0, y : y + height, x : x + width]).type(torch.LongTensor)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 10\n",
    "patch_size = (256, 256)\n",
    "samples_per_tile = 5000\n",
    "\n",
    "# Data Generators\n",
    "train_x_path = \"/mnt/blobfuse/esri-naip/v002/md/2015/md_100cm_2015/39076/m_3907639_sw_18_1_20150815.tif\"\n",
    "train_y_path = \"/mnt/blobfuse/resampled-lc/data/v1/2015/states/md/md_1m_2015/39076/m_3907639_sw_18_1_20150815_lc.tif\"\n",
    "\n",
    "test_x_path = \"/mnt/blobfuse/esri-naip/v002/md/2015/md_100cm_2015/39076/m_3907639_ne_18_1_20150815.tif\"\n",
    "test_y_path = \"/mnt/blobfuse/resampled-lc/data/v1/2015/states/md/md_1m_2015/39076/m_3907639_ne_18_1_20150815_lc.tif\"\n",
    "\n",
    "train_set = LandCoverDataset(train_x_path, train_y_path, patch_size, samples_per_tile)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "\n",
    "test_set = LandCoverDataset(test_x_path, test_y_path, patch_size, samples_per_tile)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data 0.23674941062927246\n",
      "forward pass 6.644374847412109\n",
      "Backprop 13.063219547271729\n",
      "1.519776463508606\n",
      "Loading data 0.012521982192993164\n",
      "forward pass 6.442512512207031\n",
      "Backprop 13.12864089012146\n",
      "1.4635698795318604\n",
      "Loading data 0.007647275924682617\n",
      "forward pass 6.370156526565552\n",
      "Backprop 13.006127119064331\n",
      "1.4509563446044922\n",
      "Loading data 0.032134294509887695\n",
      "forward pass 6.5572004318237305\n",
      "Backprop 12.930716753005981\n",
      "1.4614663124084473\n",
      "Loading data 0.03446459770202637\n",
      "forward pass 6.401031732559204\n",
      "Backprop 12.97642707824707\n",
      "1.4286423921585083\n",
      "Loading data 0.01968693733215332\n",
      "forward pass 6.400834560394287\n",
      "Backprop 13.051012992858887\n",
      "1.442684292793274\n",
      "Loading data 0.011702537536621094\n",
      "forward pass 6.342998027801514\n",
      "Backprop 13.070825338363647\n",
      "1.3893461227416992\n",
      "Loading data 0.012892723083496094\n",
      "forward pass 6.396114349365234\n",
      "Backprop 13.079975605010986\n",
      "1.431304693222046\n",
      "Loading data 0.017953872680664062\n",
      "forward pass 6.510049343109131\n",
      "Backprop 12.998590230941772\n",
      "1.3711506128311157\n",
      "Loading data 0.010371685028076172\n",
      "forward pass 6.423898458480835\n",
      "Backprop 12.961234331130981\n",
      "1.4158896207809448\n",
      "Loading data 0.011628389358520508\n",
      "forward pass 6.445106744766235\n",
      "Backprop 12.917988538742065\n",
      "1.3668103218078613\n",
      "Loading data 0.013927698135375977\n",
      "forward pass 6.385363340377808\n",
      "Backprop 12.968367099761963\n",
      "1.3844434022903442\n",
      "Loading data 0.014288902282714844\n",
      "forward pass 6.388718366622925\n",
      "Backprop 12.957480430603027\n",
      "1.3639053106307983\n",
      "Loading data 0.01486825942993164\n",
      "forward pass 6.406861782073975\n",
      "Backprop 12.97589635848999\n",
      "1.349979281425476\n",
      "Loading data 0.020910978317260742\n",
      "forward pass 6.400902271270752\n",
      "Backprop 13.052161693572998\n",
      "1.341672658920288\n",
      "Loading data 0.020461559295654297\n",
      "forward pass 6.417861461639404\n",
      "Backprop 13.1718008518219\n",
      "1.3564804792404175\n",
      "Loading data 0.009022712707519531\n",
      "forward pass 6.42175555229187\n",
      "Backprop 13.00108551979065\n",
      "1.3380126953125\n",
      "Loading data 0.015191316604614258\n",
      "forward pass 6.459499835968018\n",
      "Backprop 13.556031942367554\n",
      "1.3520166873931885\n",
      "Loading data 0.01544189453125\n",
      "forward pass 6.446199417114258\n",
      "Backprop 13.702762365341187\n",
      "1.3391151428222656\n",
      "Loading data 0.01533055305480957\n",
      "forward pass 6.408307790756226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/anaconda/envs/py37_pytorch/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bde030639951>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define Model Loss and Optimizers\n",
    "net = UNet(in_channels = 4, n_classes = train_set.n_classes, depth = 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    s = time.time()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_x, batch_y = data\n",
    "        t = time.time()\n",
    "        print(\"Loading data\", t - s)\n",
    "        s = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(batch_x)\n",
    "        t = time.time()\n",
    "        print(\"forward pass\", t - s)\n",
    "        s = time.time()\n",
    "\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t = time.time()\n",
    "        print(\"Backprop\", t - s)\n",
    "        s = time.time()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hook in unet to set dropout on up and down layers.\n",
    "# consider a set dropout layer function for up or down layer\n",
    "\n",
    "# create hook in unet to get layer dimensions of the outputs to each layer after conv b\n",
    "# scale dropout mask before applying it\n",
    "# consider adding \"temperature\" or learning rate adjustment to mutation and cross over types.\n",
    "# add cross over type best.\n",
    "\n",
    "# finish commenting matrix evolver.\n",
    "dropout_mask_evolver = MatrixEvolver([[3, 3]], CrossoverType.UNIFORM, MutationType.FLIP_BIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in net.parameters():\n",
    "#     print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
