{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from evolver import CrossoverType, MutationType, MatrixEvolver\n",
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tif_to_np(tif_path):\n",
    "    \"\"\"Reads a tif file and converts it into a numpy.ndarray.\n",
    "    \n",
    "    Arg:\n",
    "        tif_path: The full path to the tif file to read.\n",
    "    \n",
    "    Returns:\n",
    "        A numpy.ndarray containing the tif file data. The returned tif has a rolled\n",
    "        dimension and so the input is in the shape (channels, height width).\n",
    "    \n",
    "    \"\"\"\n",
    "    with rasterio.open(tif_path) as f:\n",
    "        return f.read()\n",
    "\n",
    "def apply_remap_values(labels, label_map):\n",
    "    \"\"\"Reassigns values inplace in an numpy array given a provided mapping.\n",
    "    \n",
    "    Args:\n",
    "        labels: An ndarray of labels.\n",
    "        label_map: A dict[int, int] mapping label classes [original, new].\n",
    "        \n",
    "    \"\"\"\n",
    "    for l1, l2 in label_map.items():\n",
    "        labels[labels == l1] = l2\n",
    "\n",
    "def sample_patch_coordinates(data, labels, patch_size, n_samples):\n",
    "    \"\"\"Generates image patches from a tile containing features and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        data: The x features of the image.\n",
    "        labels: The y labels of the image.\n",
    "        patch_size: An Iterable[int, int] size of the image patch to be extracted.\n",
    "        n_samples: The number of samples to extract per tile.\n",
    "\n",
    "    Returns:\n",
    "        A list of x_patches and y_patches containg features and labels respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    height, width = patch_size\n",
    "    channels = data.shape[0]\n",
    "    xs = np.random.randint(0, data.shape[2] - width, n_samples)\n",
    "    ys = np.random.randint(0, data.shape[1] - height, n_samples)\n",
    "    return np.dstack((xs, ys)).reshape((n_samples, 2))\n",
    "    \n",
    "class LandCoverDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Land Cover Dataset Containing patches.\"\"\"\n",
    "\n",
    "    def __init__(self, features_path, labels_path, patch_size, n_samples, patch_coordinates=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features_path: Path to the features of a tile.\n",
    "            labels_path: Path to the labels of a tile.\n",
    "            patch_size: An Iterable[int, int] size of the image patch to be extracted.\n",
    "            n_samples: The number of samples to extract per tile.\n",
    "            patch_coordinates: A list of coordinates used to identify the top left hand corners of\n",
    "                the patches to extract from the tile. If None they are randomly generated.\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = read_tif_to_np(features_path)\n",
    "        self.labels = read_tif_to_np(labels_path)\n",
    "        # Coalesces labels into 4 groups instead of 6.\n",
    "        # TODO(ameade): Consider allowing for transformation function arguments to modify data upon\n",
    "        # reading it in.\n",
    "        water_forest_land_impervious_remap = {5: 4, 6: 4}\n",
    "        apply_remap_values(self.labels, water_forest_land_impervious_remap)\n",
    "\n",
    "        self.n_classes = len(np.unique(self.labels))\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.n_samples = n_samples    \n",
    "        self.patch_coordinates = patch_coordinates\n",
    "        \n",
    "        if self.patch_coordinates is None:\n",
    "            self.patch_coordinates = sample_patch_coordinates(self.data, \n",
    "                                                              self.labels,\n",
    "                                                              self.patch_size,\n",
    "                                                              self.n_samples)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        height, width = self.patch_size\n",
    "        x, y = self.patch_coordinates[idx]\n",
    "        img = self.data[:, y : y + height, x : x + width].astype(np.float32)\n",
    "        label = self.labels[0, y : y + height, x : x + width]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 10\n",
    "patch_size = (256, 256)\n",
    "samples_per_tile = 5000\n",
    "\n",
    "# Data Generators\n",
    "train_x_path = \"/mnt/blobfuse/esri-naip/v002/md/2015/md_100cm_2015/39076/m_3907639_sw_18_1_20150815.tif\"\n",
    "train_y_path = \"/mnt/blobfuse/resampled-lc/data/v1/2015/states/md/md_1m_2015/39076/m_3907639_sw_18_1_20150815_lc.tif\"\n",
    "\n",
    "test_x_path = \"/mnt/blobfuse/esri-naip/v002/md/2015/md_100cm_2015/39076/m_3907639_ne_18_1_20150815.tif\"\n",
    "test_y_path = \"/mnt/blobfuse/resampled-lc/data/v1/2015/states/md/md_1m_2015/39076/m_3907639_ne_18_1_20150815_lc.tif\"\n",
    "\n",
    "train_set = LandCoverDataset(train_x_path, train_y_path, patch_size, samples_per_tile)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "\n",
    "test_set = LandCoverDataset(test_x_path, test_y_path, patch_size, samples_per_tile)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-fa1000417fff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 932\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2115\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m         \u001b[0;31m# dim == 3 or dim > 4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Byte"
     ]
    }
   ],
   "source": [
    "# Define Model Loss and Optimizers\n",
    "net = UNet(in_channels = 4, n_classes = train_set.n_classes, depth = 4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_x, batch_y = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_mask_evolver = MatrixEvolver([[3, 3]], CrossoverType.UNIFORM, MutationType.FLIP_BIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 256, 256])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]],\n",
       "\n",
       "        [[3, 3, 3,  ..., 2, 2, 2],\n",
       "         [3, 3, 3,  ..., 2, 2, 2],\n",
       "         [3, 3, 3,  ..., 2, 2, 2],\n",
       "         ...,\n",
       "         [3, 3, 3,  ..., 2, 2, 2],\n",
       "         [3, 3, 3,  ..., 2, 2, 2],\n",
       "         [3, 3, 3,  ..., 2, 2, 2]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 3, 3, 3],\n",
       "         [1, 1, 1,  ..., 3, 3, 3],\n",
       "         [1, 1, 1,  ..., 3, 3, 3],\n",
       "         ...,\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 3, 3, 3]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[3, 3, 3,  ..., 2, 2, 2],\n",
       "         [3, 3, 3,  ..., 2, 2, 2],\n",
       "         [3, 3, 3,  ..., 2, 2, 2],\n",
       "         ...,\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         [2, 3, 3,  ..., 3, 3, 3],\n",
       "         [2, 2, 3,  ..., 3, 3, 3]],\n",
       "\n",
       "        [[1, 1, 1,  ..., 2, 2, 2],\n",
       "         [1, 1, 1,  ..., 2, 2, 2],\n",
       "         [1, 1, 1,  ..., 2, 2, 2],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 2, 2, 2],\n",
       "         [2, 2, 2,  ..., 2, 2, 2]],\n",
       "\n",
       "        [[3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         [3, 3, 3,  ..., 3, 3, 3],\n",
       "         ...,\n",
       "         [2, 2, 2,  ..., 4, 4, 4],\n",
       "         [2, 2, 2,  ..., 4, 3, 3],\n",
       "         [2, 2, 2,  ..., 3, 3, 3]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
