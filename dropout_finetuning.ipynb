{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "from evolver import CrossoverType, MutationType, MatrixEvolver\n",
    "from models.unet import UNet\n",
    "from dataset_utils import PartitionType\n",
    "from landcover_dataloader import LandCoverDataset, create_land_cover_dataset_from_config, get_land_cover_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Networks on GPU if available.\n",
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "\n",
    "device = torch.device(dev)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all data partitions from config.\n",
    "dataset_dir = os.path.join(os.getcwd(), \"data/landcover_small\")\n",
    "create_land_cover_dataset_from_config(dataset_dir)\n",
    "\n",
    "# Create data loaders.\n",
    "dataloader_params = {\n",
    "    'batch_size': 8,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 6}\n",
    "\n",
    "train_loader = get_land_cover_dataloader(dataset_dir, PartitionType.TRAIN, dataloader_params)\n",
    "validation_loader = get_land_cover_dataloader(dataset_dir, PartitionType.VALIDATION, dataloader_params)\n",
    "finetuning_loader = get_land_cover_dataloader(dataset_dir, PartitionType.FINETUNING, dataloader_params)\n",
    "test_loader = get_land_cover_dataloader(dataset_dir, PartitionType.TEST, dataloader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, metrics, log_steps):\n",
    "    model.train()\n",
    "    metric_reports = []\n",
    "    loss_avg = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        batch_x, batch_y = data\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device) \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "        if i % log_steps == 0:\n",
    "            outputs = outputs.data.cpu().numpy()\n",
    "            batch_y = batch_y.data.cpu().numpy()\n",
    "            report = {m: metrics[m](outputs, batch_y) for m in metrics}\n",
    "            report['loss'] = loss.item()\n",
    "            metric_reports.append(report)\n",
    "            print(\"Train: \", report['loss'])\n",
    "    \n",
    "    loss_avg /= float(i)\n",
    "    return loss_avg, metric_reports\n",
    "\n",
    "def evaluate(model, loss_fn, validation_loader, metrics):\n",
    "    model.eval()\n",
    "    metric_reports = []\n",
    "    loss_avg = 0.0\n",
    "    for i, data in enumerate(validation_loader):\n",
    "        batch_x, batch_y = data\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device) \n",
    "        outputs = model(batch_x)\n",
    "        loss = loss_fn(outputs, batch_y)\n",
    "        outputs = outputs.data.cpu().numpy()\n",
    "        batch_y = batch_y.data.cpu().numpy()\n",
    "        report = {m: metrics[m](outputs, batch_y) for m in metrics}\n",
    "        report['loss'] = loss.item()\n",
    "        metric_reports.append(report)\n",
    "    \n",
    "    loss_avg /= float(i)\n",
    "    return loss_avg, metric_reports\n",
    "\n",
    "def accuracy_fn(output, labels, target=None):\n",
    "    predicts = nn.functional.softmax(outputs, dim=1).argmax(dim=1)\n",
    "\n",
    "    if target is None:\n",
    "        # Calculate accuracy over all classes.\n",
    "        return np.sum(predicts == labels) / np.prod(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache and create model.\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "params = {\n",
    "    'max_epochs': 5,\n",
    "    'n_classes': 4,\n",
    "    'in_channels': 4,\n",
    "    'depth': 4,\n",
    "    'learning_rate': 0.01,\n",
    "    'momentum': 0.8,\n",
    "    'log_steps': 25\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    \"accuracy\": accuracy_fn\n",
    "}\n",
    "    \n",
    "model = UNet(in_channels = params['in_channels'],\n",
    "           n_classes = params['n_classes'],\n",
    "           depth = params['depth'])\n",
    "model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=params['learning_rate'],\n",
    "                            momentum=params['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune with back prop\n",
    "for epoch in range(params['max_epochs']):\n",
    "    train_loss, train_reports = train(model, optimizer, loss_fn,\n",
    "                                      train_loader, metrics, params['log_steps'])\n",
    "    validate_loss, validate_reports = evaluate(model, loss_fn, validation_loader, metrics)\n",
    "\n",
    "for epoch in range(params['max_epochs']):\n",
    "    finetuning_loss, finetuning_reports = train(model, optimizer, loss_fn,\n",
    "                                                finetuning_loader, metrics, params['log_steps'])\n",
    "    test_loss, test_reports = evaluate(model, loss_fn, test_loader, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune with learned dropout\n",
    "dropout_masks = {\n",
    "    'start': [256, 256],\n",
    "#     'down_0': None,\n",
    "#     'down_1': None,\n",
    "#     'down_2': None,\n",
    "#     'down_3': None,\n",
    "#     'down_4': None,\n",
    "#     'up_0': None,\n",
    "#     'up_1': None,\n",
    "#     'up_2': None,\n",
    "#     'up_3': None,\n",
    "#     'end': None,   \n",
    "}\n",
    "\n",
    "finetuning_params = {\n",
    "    \"n_generations\": 100\n",
    "    \"n_children\": 10\n",
    "}\n",
    "\n",
    "evolver = MatrixEvolver([m for k, m in dropout_masks.items() if m is not None],\n",
    "                        CrossoverType.UNIFORM, MutationType.FLIP_BIT)\n",
    "\n",
    "model.eval()\n",
    "for generation in range(finetuning_params['n_generations']):\n",
    "    for child in range(finetuning_params['n_children'])\n",
    "        child_masks = evolver.spawn_child()\n",
    "        model.set_dropout_masks({k: torch.tensor(child_masks[i], \n",
    "                                                 device=device,\n",
    "                                                 dtype=torch.float) for i, k in enumerate(dropout_masks.keys())})\n",
    "        total_loss = 0\n",
    "        for i, data in enumerate(finetuning_loader):\n",
    "            batch_x, batch_y = data\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = loss_fn(outputs, batch_y)\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        evolver.add_child(child_masks, total_loss)\n",
    "    \n",
    "    evolver.update_parents()\n",
    "\n",
    "    \n",
    "child_masks = evolver.spawn_child()\n",
    "model.set_dropout_masks({k: torch.tensor(child_masks[i], \n",
    "                                         device=device,\n",
    "                                         dtype=torch.float) for i, k in enumerate(dropout_masks.keys())})\n",
    "test_loss, test_reports = evaluate(model, loss_fn, test_loader, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
