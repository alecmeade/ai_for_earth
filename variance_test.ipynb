{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import sys\n",
    "\n",
    "# To view tensorboard metrics\n",
    "# tensorboard --logdir=logs --port=6006 --bind_all\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "from evolver import CrossoverType, MutationType, InitType, MatrixEvolver, VectorEvolver\n",
    "from unet import UNet\n",
    "from dataset_utils import PartitionType\n",
    "from cuda_utils import maybe_get_cuda_device, clear_cuda\n",
    "from landcover_dataloader import get_landcover_dataloaders, get_landcover_dataloader\n",
    "\n",
    "from ignite.contrib.handlers.tensorboard_logger import *\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, ConfusionMatrix, mIoU\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.utils import setup_logger\n",
    "from ignite.engine import Engine\n",
    "\n",
    "# Define directories for data, logging and model saving.\n",
    "base_dir = os.getcwd()\n",
    "dataset_name = \"landcover_large\"\n",
    "dataset_dir = os.path.join(base_dir, \"data/\" + dataset_name)\n",
    "\n",
    "experiment_name = \"dropout_single_point_finetuning_100_children_variance\"\n",
    "model_name = \"best_model_9_validation_accuracy=0.8940.pt\"\n",
    "model_path = os.path.join(base_dir, \"logs/\" + dataset_name + \"/\" + model_name)\n",
    "log_dir = os.path.join(base_dir, \"logs/\" + dataset_name + \"_\" + experiment_name)\n",
    "\n",
    "# Create DataLoaders for each partition of Landcover data.\n",
    "dataloader_params = {\n",
    "    'batch_size': 8,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 6,\n",
    "    'pin_memory': True}\n",
    "\n",
    "partition_types = [PartitionType.TRAIN, PartitionType.VALIDATION, \n",
    "                   PartitionType.FINETUNING, PartitionType.TEST]\n",
    "data_loaders = get_landcover_dataloaders(dataset_dir, \n",
    "                                         partition_types,\n",
    "                                         dataloader_params,\n",
    "                                         force_create_dataset=False)\n",
    "\n",
    "\n",
    "train_loader = data_loaders[0]\n",
    "finetuning_loader = data_loaders[2]\n",
    "\n",
    "dataloader_params['shuffle'] = False\n",
    "test_loader = get_landcover_dataloader(dataset_dir, PartitionType.TEST, dataloader_params)\n",
    "\n",
    "\n",
    "# Get GPU device if available.\n",
    "device = maybe_get_cuda_device()\n",
    "\n",
    "# Determine model and training params.\n",
    "params = {\n",
    "    'max_epochs': 10,\n",
    "    'n_classes': 4,\n",
    "    'in_channels': 4,\n",
    "    'depth': 5,\n",
    "    'learning_rate': 0.001,\n",
    "    'log_steps': 1,\n",
    "    'save_top_n_models': 4,\n",
    "    'num_children': 100\n",
    "}\n",
    "\n",
    "clear_cuda()    \n",
    "model = UNet(in_channels = params['in_channels'],\n",
    "             n_classes = params['n_classes'],\n",
    "             depth = params['depth'])\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# Create Trainer or Evaluators\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=params['learning_rate'])\n",
    "\n",
    "# Determine metrics for evaluation.\n",
    "metrics = {\n",
    "        \"accuracy\": Accuracy(), \n",
    "        \"loss\": Loss(criterion),\n",
    "        \"mean_iou\": mIoU(ConfusionMatrix(num_classes = params['n_classes'])),\n",
    "}\n",
    "\n",
    "for batch in train_loader:\n",
    "    batch_x = batch[0]\n",
    "    _ = model(batch_x)\n",
    "    break\n",
    "    \n",
    "drop_out_layers = model.get_dropout_layers()\n",
    "del model, batch_x\n",
    "clear_cuda()\n",
    "\n",
    "for layer in drop_out_layers:\n",
    "    layer_name = layer.name\n",
    "    size = layer.x_size[1:]\n",
    "    sizes = [size]\n",
    "    clear_cuda()    \n",
    "    model = UNet(in_channels = params['in_channels'],\n",
    "                 n_classes = params['n_classes'],\n",
    "                 depth = params['depth'])\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), \n",
    "                                 lr=params['learning_rate'])\n",
    "    \n",
    "    num_channels = size[0]\n",
    "    evolver = VectorEvolver(num_channels, \n",
    "                            CrossoverType.UNIFORM,\n",
    "                            MutationType.FLIP_BIT, \n",
    "                            InitType.RANDOM, \n",
    "                            flip_bit_prob=0.25, \n",
    "                            flip_bit_decay=0.5)\n",
    "\n",
    "    log_dir_test = log_dir + \"_\" + layer_name\n",
    "    \n",
    "    def mask_from_vec(vec, matrix_size):\n",
    "        mask = np.ones(matrix_size)\n",
    "        for i in range(len(vec)):\n",
    "            if vec[i] == 0:\n",
    "                mask[i, :, :] = 0\n",
    "\n",
    "            elif vec[i] == 1:\n",
    "                mask[i, :, :] = 1\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def dropout_finetune_step(engine, batch):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(params['num_children']):\n",
    "                model.zero_grad()\n",
    "                child_vec = evolver.spawn_child()\n",
    "                child_mask = mask_from_vec(child_vec, size)\n",
    "                model.set_dropout_masks({layer_name: torch.tensor(child_mask, dtype=torch.float32).to(device)})\n",
    "                outputs = model(batch_x)\n",
    "                current_loss = criterion(outputs[:, :, 127:128,127:128], batch_y[:,127:128,127:128]).item()\n",
    "                loss = min(loss, current_loss)\n",
    "                \n",
    "                if current_loss == 0.0:\n",
    "                    current_loss = sys.float_info.max\n",
    "                else:\n",
    "                    current_loss = 1.0 / current_loss\n",
    "\n",
    "                evolver.add_child(child_vec, current_loss)\n",
    "                \n",
    "            priority, best_child = evolver.get_best_child()\n",
    "            best_mask = mask_from_vec(best_child, size)\n",
    "            \n",
    "            model.set_dropout_masks({layer_name: torch.tensor(best_mask, dtype=torch.float32).to(device)})\n",
    "            return loss\n",
    "\n",
    "    # Create Trainer or Evaluators\n",
    "    trainer = Engine(dropout_finetune_step)\n",
    "    train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "    validation_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n",
    "    trainer.logger = setup_logger(\"Trainer\")\n",
    "    train_evaluator.logger = setup_logger(\"Train Evaluator\")\n",
    "    validation_evaluator.logger = setup_logger(\"Validation Evaluator\")\n",
    "\n",
    "    @trainer.on(Events.ITERATION_COMPLETED(every=1))\n",
    "    def report_evolver_stats(engine):\n",
    "        priorities = np.array(evolver.get_generation_priorities())\n",
    "        # Take reciprocal since we needed to store priorities in min heap.\n",
    "        priorities = 1.0 / priorities\n",
    "        tb_logger.writer.add_scalar(\"training/evolver_count\",\n",
    "                                    priorities.shape[0], engine.state.iteration)\n",
    "        tb_logger.writer.add_scalar(\"training/evolver_mean\",\n",
    "                                    np.mean(priorities), engine.state.iteration)\n",
    "        tb_logger.writer.add_scalar(\"training/evolver_std\",\n",
    "                                    np.std(priorities), engine.state.iteration)\n",
    "        evolver.update_parents()\n",
    "       \n",
    "\n",
    "        \n",
    "    # Tensorboard Logger setup below based on pytorch ignite example\n",
    "    # https://github.com/pytorch/ignite/blob/master/examples/contrib/mnist/mnist_with_tensorboard_logger.py\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def compute_metrics(engine):\n",
    "        \"\"\"Callback to compute metrics on the train and validation data.\"\"\"\n",
    "        validation_evaluator.run(test_loader)\n",
    "\n",
    "    def score_function(engine):\n",
    "        \"\"\"Function to determine the metric upon which to compare model.\"\"\"\n",
    "        return engine.state.metrics[\"accuracy\"]\n",
    "\n",
    "    # Setup Tensor Board Logging    \n",
    "    tb_logger = TensorboardLogger(log_dir=log_dir_test)\n",
    "\n",
    "    tb_logger.attach_output_handler(\n",
    "        trainer,\n",
    "        event_name=Events.ITERATION_COMPLETED(every=params['log_steps']),\n",
    "        tag=\"training\",\n",
    "        output_transform=lambda loss: {\"batchloss\": loss},\n",
    "        metric_names=\"all\",\n",
    "    )\n",
    "\n",
    "    for tag, evaluator in [ (\"validation\", validation_evaluator)]:\n",
    "        tb_logger.attach_output_handler(\n",
    "            evaluator,\n",
    "            event_name=Events.EPOCH_COMPLETED,\n",
    "            tag=tag,\n",
    "            metric_names=\"all\",\n",
    "            global_step_transform=global_step_from_engine(trainer),\n",
    "        )\n",
    "\n",
    "    tb_logger.attach_opt_params_handler(trainer, \n",
    "                                        event_name=Events.ITERATION_COMPLETED(every=params['log_steps']), \n",
    "                                        optimizer=optimizer)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        log_dir_test,\n",
    "        n_saved=params['save_top_n_models'],\n",
    "        filename_prefix=\"best\",\n",
    "        score_function=score_function,\n",
    "        score_name=\"validation_accuracy\",\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "\n",
    "    validation_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {\"model\": model})\n",
    "    trainer.run(finetuning_loader, max_epochs=params['max_epochs'])\n",
    "    tb_logger.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
